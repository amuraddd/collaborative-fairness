{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 513,
   "id": "126146ee-1652-4732-91b7-849974550c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "id": "45a0fe90-0df6-4116-a7cd-2c7a118bf1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"LunarLander-v2\", render_mode=\"human\")\n",
    "observation, info = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "id": "cf36a871-6b7a-459a-a564-aa6f2706e08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(1000):\n",
    "    action = env.action_space.sample()\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "    \n",
    "    if terminated or truncated:\n",
    "        observation, info = env.reset()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "id": "52dd8414-6675-4c03-b9d9-2c7538e4e36b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-2.8145547768651227"
      ]
     },
     "execution_count": 516,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1201a3b8-f7e6-445a-a5b5-0c1e6560704f",
   "metadata": {},
   "source": [
    "### Hands on | CH 18 Reinforcement Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2f6ecdb-8dff-4833-bfbf-bf6f5a4abc98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e05c108c-1388-4609-a38d-cca569b47cb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['CartPole-v0', 'CartPole-v1', 'MountainCar-v0', 'MountainCarContinuous-v0', 'Pendulum-v1', 'Acrobot-v1', 'CartPoleJax-v0', 'CartPoleJax-v1', 'PendulumJax-v0', 'LunarLander-v2', 'LunarLanderContinuous-v2', 'BipedalWalker-v3', 'BipedalWalkerHardcore-v3', 'CarRacing-v2', 'Blackjack-v1', 'FrozenLake-v1', 'FrozenLake8x8-v1', 'CliffWalking-v0', 'Taxi-v3', 'Reacher-v2', 'Reacher-v4', 'Pusher-v2', 'Pusher-v4', 'InvertedPendulum-v2', 'InvertedPendulum-v4', 'InvertedDoublePendulum-v2', 'InvertedDoublePendulum-v4', 'HalfCheetah-v2', 'HalfCheetah-v3', 'HalfCheetah-v4', 'Hopper-v2', 'Hopper-v3', 'Hopper-v4', 'Swimmer-v2', 'Swimmer-v3', 'Swimmer-v4', 'Walker2d-v2', 'Walker2d-v3', 'Walker2d-v4', 'Ant-v2', 'Ant-v3', 'Ant-v4', 'Humanoid-v2', 'Humanoid-v3', 'Humanoid-v4', 'HumanoidStandup-v2', 'HumanoidStandup-v4', 'GymV22Environment-v0', 'GymV26Environment-v0'])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## list of all environments in gym\n",
    "gym.envs.registry.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "id": "91badfea-6e6b-406c-a9cf-e2e1ba9648b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-0.02758559, -0.00663392, -0.02858965,  0.01628222], dtype=float32),\n",
       " {})"
      ]
     },
     "execution_count": 517,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
    "obs = env.reset() #this initializes the environment\n",
    "obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "c35c4fdc-000c-4ad1-8d53-5109f6b57cd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(2)"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# env.render()\n",
    "env.action_space #possible actions in the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66af23c-18ce-40d1-bb7e-7bb25b68af6c",
   "metadata": {},
   "source": [
    "- Discreate(2) means that either action 1 or 0 is possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "id": "1892e850-7b51-40ea-a10e-86cadbf805f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "action = 1\n",
    "obs, reward, done, _, info = env.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "id": "44fdf11e-77e4-4b9c-8b92-0d7739e6db46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.02771827,  0.18888612, -0.028264  , -0.28528222], dtype=float32)"
      ]
     },
     "execution_count": 519,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "id": "e3f705ac-0fa4-4b37-8d7a-f9c8316f11bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_policy(obs):\n",
    "    angle = obs[2]\n",
    "    return 0 if angle < 0 else 1\n",
    "\n",
    "totals = []\n",
    "for episode in range(500):\n",
    "    episode_rewards = 0\n",
    "    obs = env.reset()[0]\n",
    "    for step in range(200):\n",
    "        action = basic_policy(obs)\n",
    "        obs, reward, done, _, info = env.step(action)\n",
    "        episode_rewards += reward\n",
    "        if done:\n",
    "            break\n",
    "    totals.append(episode_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "id": "9d256590-c154-495b-a060-d08d3877215b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "54219b35-0be9-4986-8101-d3dc4cbf2dce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41.69, 9.29440154071256, 68.0, 24.0)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.mean(totals), np.std(totals), np.max(totals), np.min(totals)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60db9088-b43b-42ba-84bc-7bcb6d34cdc7",
   "metadata": {},
   "source": [
    "## Deep Reinforcement Learning\n",
    "- Instead of hard coding policies use a neural network to learn a policy.\n",
    "    - Input: observation given the current state\n",
    "    - Output: probability of the action to be taken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "7b20245e-685f-4a0f-be0c-0be5f06ad6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "ac198ccb-24a0-42e9-91d3-28f395ed14b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNN(nn.Module):\n",
    "    def __init__(self,\n",
    "                input_shape:int,\n",
    "                hidden_units:int,\n",
    "                output_shape: int):\n",
    "        super().__init__()\n",
    "        self.layer_stack = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features=input_shape,\n",
    "                     out_features=hidden_units),\n",
    "            nn.Linear(in_features=hidden_units,\n",
    "                     out_features=output_shape)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return torch.sigmoid(self.layer_stack(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "id": "8b0851a0-537b-4c8c-9c2a-4ef27d86a102",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute gradients and save them\n",
    "def play_one_step(env, obs, model, loss_fn):\n",
    "    \"\"\"\n",
    "    Play one step and compute the gradients\n",
    "    Generate new obs, reward etc.\n",
    "    \"\"\"\n",
    "    obs = torch.tensor(obs[np.newaxis])\n",
    "    left_proba = model(obs)\n",
    "    action = (torch.FloatTensor(1,1).uniform_(0,1) > left_proba)\n",
    "    action = action.clone().detach().type(torch.float32)\n",
    "    y_target = torch.ones(1,1) - action\n",
    "    loss = loss_fn(left_proba, y_target)\n",
    "\n",
    "    loss.backward() #compute gradients\n",
    "    grads = []\n",
    "    for param in model.parameters():\n",
    "        grads.append(param.grad.view(-1))\n",
    "    grads = torch.cat(grads)    \n",
    "    obs, reward, done, _, info = env.step(int(action.numpy()[0]))\n",
    "    return obs, reward, done, grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "id": "a10820ca-3a79-4117-ac79-c70aa5d70f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_multiple_episodes(env, n_episodes, n_max_steps, model, loss_fn):\n",
    "    \"\"\"\n",
    "    Play multiple episodes.\n",
    "    \"\"\"\n",
    "    all_rewards = []\n",
    "    all_grads = []\n",
    "    for episode in range(n_episodes):\n",
    "        current_rewards = []\n",
    "        current_grads = []\n",
    "        obs = env.reset()[0]\n",
    "        for step in range(n_max_steps):\n",
    "            obs, reward, done, grads = play_one_step(env, obs, model, loss_fn)\n",
    "            current_rewards.append(reward)\n",
    "            current_grads.append(grads)\n",
    "            if done:\n",
    "                break\n",
    "        all_rewards.append(current_rewards)\n",
    "        all_grads.append(current_grads)\n",
    "    return all_rewards, all_grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "id": "e8e919eb-515a-4591-baa3-c663d853f154",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_rewards(rewards, discount_factor):\n",
    "    discounted = np.array(rewards)\n",
    "    for step in range(len(rewards)-2, -1, -1):\n",
    "        discounted[step] += discounted[step+1] * discount_factor\n",
    "    return discounted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "id": "e89c6e42-a65d-46c4-8e09-992c7fe26c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_and_normalize_rewards(all_rewards, discount_factor):\n",
    "    all_discounted_rewards = [discount_rewards(rewards, discount_factor)\n",
    "                              for rewards in all_rewards]\n",
    "    flat_rewards = np.concatenate(all_discounted_rewards)\n",
    "    reward_mean = flat_rewards.mean()\n",
    "    rewards_std = flat_rewards.std()\n",
    "    return [(discounted_rewards - reward_mean)/rewards_std\n",
    "           for discounted_rewards in all_discounted_rewards]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "id": "a6e3d89f-93cf-494e-b41d-9faacd6d2729",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([-0.28435071, -0.86597718, -1.18910299]),\n",
       " array([1.26665318, 1.0727777 ])]"
      ]
     },
     "execution_count": 395,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "discount_rewards([10,0,-50], discount_factor=0.8)\n",
    "discount_and_normalize_rewards([[10,0,-50], [10, 20]],\n",
    "                              discount_factor=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "id": "7c89467c-e944-4fbe-bc95-46217a93a58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PolicyNN(input_shape=4,\n",
    "                hidden_units=10,\n",
    "                output_shape=1)\n",
    "\n",
    "loss_fn = nn.BCELoss() #loss function\n",
    "optimizer = torch.optim.SGD(model.parameters(), #optimizer\n",
    "                           lr=0.01)\n",
    "\n",
    "#hyperparameters\n",
    "n_iterations = 150\n",
    "n_episodes_per_update = 10\n",
    "n_max_steps = 200\n",
    "discount_factor = 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "id": "ba59aeff-b88b-4f89-8dc4-bac4f946d9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
    "obs = env.reset() #this initializes the environment\n",
    "action = 1\n",
    "obs, reward, done, _, info = env.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "id": "fe903875-a38f-4657-b1ab-edfb51cc016c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# _,_,_,w = play_one_step(env, obs, model, loss_fn)\n",
    "# w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "id": "7edf7a7f-2b57-44e0-b248-6b25726c588e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.reset()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "id": "cb46114f-f198-4ce5-aaa0-ecb2dd840164",
   "metadata": {},
   "outputs": [],
   "source": [
    "### training loop\n",
    "for iteration in range(n_iterations):\n",
    "    all_rewards, all_grads = play_multiple_episodes(\n",
    "        env, n_episodes_per_update, n_max_steps, model, loss_fn)\n",
    "    all_final_rewards = discount_and_normalize_rewards(all_rewards,\n",
    "                                                      discount_factor)\n",
    "    all_mean_grads = []\n",
    "    for var_index in range(sum(p.numel() for p in model.parameters() if p.requires_grad)):\n",
    "        mean_grads = [torch.tensor(final_rewards)*all_grads[episode_index][step][var_index] \n",
    "             for episode_index, final_rewards in enumerate(all_final_rewards) \n",
    "                 for step, final_reward in enumerate(final_rewards)]#, dim=0)\n",
    "        all_mean_grads.append(mean_grads)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "id": "bab45de3-676b-4d6c-b49b-b8fa696d68ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [final_rewards*np.array(all_grads[episode_index][step][var_index])\n",
    "#             for episode_index, final_rewards in enumerate(all_final_rewards)\n",
    "#                 for step, final_reward in enumerate(final_rewards)]\n",
    "# all_mean_grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "id": "b309cd59-b05c-4d36-9040-3406416084b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "id": "9df67f3a-c840-48f8-8810-a811f88d7393",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace54d6e-bb1d-46e7-b9ba-0dea6f1cadb1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
